# ML Lifecycle Notes
[Deck](https://docs.google.com/presentation/d/12UCPtPBCtPBDxhgVVR54Ze-XGqCpiSSnjEiNXJw-3rs/edit?usp=sharing)

## HISTORICAL DASHBOARD
Starting from the beginning here, here’s our existing dashboard. This is the state of the world today. We have historical analysis and churn rate information. 

Where we want to go is to be able to build a predictive model to understand which customers will churn. Right now I’m in SQL Analyt lens of Databricks so I’m going to flip over to the machine learning side of the product and we can look at the notebooks for this.
[Historical Dashboard](https://adb-984752964297111.11.azuredatabricks.net/sql/dashboards/33d5346c-f7a7-4a97-9b24-5b86b5d59b4a-telco-churn-dashboard---historical?o=984752964297111)

## NOTEBOOKS LIST
* Not ETL or EDA 

* Sean Owen and Austin Ford’s talk on exploratory data analysis and data science on Databricks

## FEATURE STORE
* Check for features in the feature store

* The value of the feature store is in organization and reduction of duplicative work.

## 01_FEATURE_ENGINEERING !! (run all at top) !!

* Featurization is the process of using varibles to inform the model about the world. In this case, the world consists of customers. So we are teaching the models about the customers.

* Can everyone see this ok or do I need to make it bigger?

* Model do not understand words like gender so we use a technique called one-hot encoding to create a binary variable is_female yes or no

Define featurization function
* I’m using the feature store capability that has been recently announced for Databricks. 
* I’m going to define a function that performs one-hot-encoding. 
* In this function I’m using Koalas. Has anyone on the call ever used Koalas before?
* Koalas is a Pandas' API on Spark
* This is valuable because Pandas is a very common Python package for working with data. The syntax for Pandas is the same as the syntax for Koalas. Meaning you can run the same code written for a single machine on Spark taking advantage of its parallelism. The difference is in the import. We import koalas instead of pandas.


Write features to the feature store
* Here we are computing those features and saving them to the feature store.
* Before feature stores tracking features was an adventure of never up to date documentation. How are features stored, tracked, and reused now?

* Notice we are including a key and a description. This allows other people to understand what column the features are bulit with respect to when they find them in the feature store.


* Now we could just open up another notebook and we could start writing code. However, it is very efficient and I think a tremendous boost to our productivity to use the AutoML that is now built into Databricks. 

## AUTOML
* new AutoML experiment
* The default timeout is 60 and the maximum of trial’s 200. We can go all the way down to five minutes. Let’s get this going and let’s see what happens.

## WAITING
* We’re sitting here inside the tracking server and we’re waiting for runs to come in. 
* Right now, AutoML is taking the data that we gave it and it is using Spark and HyperOpt to parallelize the search for the best model. HyperOpt, if you’re not familiar, is an open source library that uses Bayesian optimization to try to converge on what the best combination of parameter values are for each type of algorithm.
* AutoML is going to going to parallelize that search using Spark and we’ll do so across a number of different models as you’ll see. 
* We’ll be exploring XGBoost, logistic regression and random forest. 
* MLFlow is a huge time saver. I used to keep track of my features, algorithms, and their respective parameters in spreadsheets and eventually tables. 
* How do you track models today?

* The huge producitivty boost that comes from the AutoML is in the glass box approach. Not only are we optimizing the parameters for mutiple alogithms in parallel we are doing in it 10 mins which would normally take weeks.
* Then the glass box gives us full transparency. 

## DATA EXPLORATION NOTEBOOK
* Take the autogenerated eda notebook for example. 
* It is a good starting point if you haven't done so yet. This isn’t to say that you shouldn’t do your due diligence and do exploratory data analysis but the way that Databricks has built AutoML is with a glass box approach. 
* You’ll be able to see exactly what’s going on. We generate all the code, we generate all the notebooks for you and then we give them to you so that you can open them up and edit them. 
* This is huge contrast to something like a black box approach, where you push a button and you get a model out of it. It could be a really good model, but you have no way to go in inspect it or edit it.
                                 
* In the eda notebook, we have summaries of the data and visualizations. 

## MLFLOW TRACKING SERVER
* Different models that are being trained with different parameters

* It will keep going until we either converge upon the best notebook, the best trial, or we run out of time.

## SPECIFIC RUN
* MLflow auto logging. Meaning all of these parameters associated with this particular model training, model validation, and all of the metrics in terms of its performance are being logged automatically. 

* Tagging It’s been tagged with the estimator class and the estimator name so far. 

* AutoML on Databricks automatically produces the most common evaluation images: confusion matrix, the precision recall curve and the ROC curve. 

* Model schema. The model schema or the model signature is essentially the definition of what the model expects in terms of inputs and outputs. The reason for this is: let's say you have a model running in production and you want to update it with a new version but your schemas don’t match. Without schema enforcement you’re going to break something. Having this captured at the time that the model is logged is a safeguard and it allows you to have confidence that your pipeline will be more stable.

* Dependencies and the system environment for the particular model are also logged along with the model. 

## 02_AUTOML_BASELINE
Lets take a minute to look at what the notebooks look like associated with AutoML models. This is a particular notebook that was generated from a run  (plus a picture, quote, and at note)
I added the customer quote because our AutoML capability is fairly new, but been tested and approved by customers already.
This auto-generated notebook is exactly what we mean by a glass box AutoML. Each step in the modeling process is in here and editable. I was a data scientist for years and this would have been a huge productivity boost. Now, scientists can focus refinement and creative featurization rather than rewriting the same code just a little bit differently. This is totally transparent. 

One of the things I think is pretty cool is that we actually use SHAP as part of this auto-generated notebooks, to calculate the feature importance of the variables in your training data. Now, just to note here, by default, we’ll only use one sample. I’ve actually already come in here and edited this. For the sake of expediency, the setting here is to just use one sample to generate the SHAP values, but you can come in here and increase it. That’ll just increase the runtime. But you’ll be able to see which variables are most impactful in terms of predicting churn. 


We have our feature engineering pipeline done our baseline AutoML model. 
Now we will focus on MLOps.

## ------------------ HALF WAY ---------------


## 03_WEBHOOKS_SETUP (do not run)

* If you are not familiar with webhooks, they’re essentially HTTP requests that are triggered when an event happens. Think of it a door bell. You push it and it makes a sound. If you register a model, it sends a message to slack.

* Here is a list of MLFlow supported events

* Two primary types of webhooks that we’re going to be using : We’re going to use a webhook to send notifications to slack and another one to trigger the testing job. 

* So how is that going to work? So let’s take a look at this diagram here. So there’s two notebooks here. 

	* We have a promote best run to registry notebook that triggers a slack message and the testing job
	
	* We have a testing notebook that triggers a slack message with the results of the test


## Jobs 
* Churn model validation job - we need this job id in order instruct a webhook to launch this job when a model is requested to staging
* Also, note the cluster assigned uses a ML runtime. Your job will fail with an import error for mlflow without an ML runtime.

## 03_WEBHOOKS_SETUP (return)


* This is how you create a webhook. 

	* I’m not going to go into too, too much detail, but we’re going to get the model name from the widget up here and then we’re going to create some json that we will post to the MLflow REST API.  
	* In our request, 
		* we’re going to specify the model name, 
		* the type of event that we want the webhook to fire on
		* We’ll give it a description 
		* Last we have to add the job ID - recall we got this from the jobs tab
	* We just make that post request and pass that body, that jets on to the end point and then the webhook is created.

* Similarly, when we set up the slack notification, we do it the same way. The only difference here is that we have a web URL that we get from slack and then we include that in our webhook requests to create the webhook. 

* You can also list & delete webhooks - it is in here for housekeeping

Now that we have created our webhooks lets go to our notebook that registers our model.

## 04_from_exp_to_registry (RUN ALL)

In this notebook we’re going to register the model, annotate the model, and then request to transition it to staging. 

We are going to promote our best autoML run to the registry. In practice, we would use our experience to refine and improve the autoML model. We will assume that is done by the data scientist after the model is in staging.

### What is the Model Registry?
* The model registry is a centralized model repository. 
* It provides chronolgical model lineage, model versioning, stage transitions, and email notifications of model events. 
* The model registry removes the need for spreadsheet of models and all of the information surrounding them providing a smoother, reproducable, and productive method for the model life cycle than any manual tracking method.
* You can work with the model registry using either the Model Registry UI or API
* You can easily lift experiments out of the tracking server and put it into a central place.
* Why use the model registry if we already have git?
	* Git tracks code
	* The model registry tracks models and the artifacts associated with them meaning the code that generated the model, the parameters and features of the model, the evaluation artifacts, all the metrics associated with the model, and the model itself.
	
Questions?

* Okay, so the first step here is to actually take the model from the run ID from the best performing experiment. Normally this would be after we had improved on the autoML model so I'll just point out where to get the run ID from the experiments tab of our autoML because we haven't run any other experiements.

* Next we tag the experiment run. 
	* Tagging is completely custom. 
	* I am going to tag what the features table was that this model was trained on and the demographic variables used. 
	* We will use this later to check to see whether or not every permutation of demographic variables have an acceptable evaluation metric.
	* These are just examples

* Mostly I tag the model not the run, but this way you have examples of both.

* This registers the model. You can see the version here.

* Now we’re going to update the description, In terms of best practices, documentation is a must. We want to reduce duplicative work and make our model easy to find and be used by other teams.
* The testing job requires a description to push models to the registry. 

* We’re connecting to the API to submit a request to transition and leave a comment.

* The request to staging fired off two webhooks:
	* First, we should be able to see in slack that we have a notification.
	* Second, we should see our job kicked off in the jobs tab that will test the model 

## Scheduled Jobs

* The churn model validation job is running our ops notebook.  

* We can open the notebook from here.

* The ML engineer wrote this notebook. We are in the testing section.

* Here connect to the feature store and mlflow to pull model details

* Walk through some checks
	* Predictions - We will tag this model in the registry as it can make predictions and vice versa if it cannot. 
 	* Signature or Schema - check to make sure that the schema has been included with the model.
 	* Accuracy - Use the previously passed variables to test accuracy for the provided demographic segments.
 	* Description - check to see if the description is included
 	* Artifacts - storing the artifacts were stored as part of the experiment run in the tracking server. So we can always go back to look at them. This is important for reproducibility and audit ability. 

* Test results

* Send test results to slack and either ACCEPT or REJECT the model transition

## Check slack for webhook messages


Now that was a whole bunch of stuff that we did in the model registry, so let’s go take a look at all the things that happened there before we move on.

## Model Registry
* Here is our model version x
* We have a description of the model. 

* Let’s see, what’s going on with our latest version. 
* There’s the input schema and 
* the results of our webhooks

We have seen our webhooks function, registered, tested, and successfully staged our model now we are ready to do inference.

## 06 Inference

Let’s go over to our inference notebook. This was created by our data engineer. This notebook is short and sweet.

* What we’re going to do is we just load in the model using a Spark User Defined Function or UDF using the model uri. If we update the version of the model and the registry that is marked as staging, then this is will always refer to the latest version of that.

* Load features 

* Apply the model

* Write these predictions to Delta

## Predictive Dashboard
* Our data scientist has updated the dashboard to include predictive information.

* We can now see that 24% of our monthly revenue is at risk for churn

* Next we are time traveling into the future we have compared our predictions to reality for a couple months

* Our stakeholders have approved the dashboard and are ready to begin actioning off of it. 

* As a result, we are going to move our model into production.

## Model Registry
We are going to use the UI to request our staging model be moved into production. 


## 07 Retrain Churn AutoML
Our ML engineer has setup this notebook to rerun autoML every month so our data scientist can focus on refinement and featurization rather than kicking autoML.

* Walk through

## Job Scheduler
Okay, now the last thing we need to do now that we have this retrain job is we need to schedule it to run on a monthly basis

* Create job
	* Update task name
	* Create another task - features
	* Create another task - autoML 
* In our monthly retrain notebook we request to staging. Which does what?
	* It kicks off our testing job and sends a slack message
	* What happens if our model passes the tests?
	* The model is pushed to staging
* Edit schedule
* Add an alert

That a long yet rapid end-to-end walkthrough of how you could go from feature engineering all the way through to model deployments


## Wrap Up Slide
Another way of looking at this if we lose the abstraction and we look at it more from Databricks features and product focus is, what we really did was walk through the whole full ML Lifecycle as three different personas all working on the same platform. 

* We accomplished
	* data prep and featurization
	* model developments using Databricks glassbox AutoML
	* we leveraged MLFlow's tracking and model registry
	* and lastly put our model into production

If you want to learn more about this kind of approach to MLOps and ML Engineering, the field team here at Databricks has been releasing blogs and other content on this this year. Some guest authors from Standford just published a guest post on the Databricks blog about their new model assersions library that checks for post prediction outliers. 

Thank you for coming any additional questions?


